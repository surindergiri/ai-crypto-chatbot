# Reads documents (PDF / TXT / MD)
# Breaks them into chunks
# Converts chunks → vectors (embeddings)
# Stores them in ChromaDB
# Searches them when a question is asked
# Returns relevant text to the AI

import os
import json
import chromadb
from chromadb.utils import embedding_functions
from pypdf import PdfReader

# ---------------------------------------------------------
# Configuration
# ---------------------------------------------------------
CHROMA_DB_PATH = "./chroma_db"
COLLECTION_NAME = "crypto_knowledge"
DATA_FOLDER = "./data"

# Global client/collection reference
_collection = None

def initialize_rag():
    """Initializes ChromaDB (Latest) and ingests any new files."""
    global _collection
    
    print("Initializing RAG Service (ChromaDB Latest)...")
    
    try:
        # 1. Setup Client (Persistent)
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        
        # 2. Setup Embedding Function (Default is all-MiniLM-L6-v2)
        embed_fn = embedding_functions.DefaultEmbeddingFunction()
        
        # 3. Get/Create Collection
        _collection = client.get_or_create_collection(
            name=COLLECTION_NAME, 
            embedding_function=embed_fn
        )
        # If collection exists → load it
        # If not → create it
        
        print(f"RAG: Collection '{COLLECTION_NAME}' loaded. Count: {_collection.count()}")
        
        # 4. Ingest Data Folder
        if os.path.exists(DATA_FOLDER):
            _ingest_folder(DATA_FOLDER)
        else:
            os.makedirs(DATA_FOLDER, exist_ok=True)

        # This scans and ingests all documents inside ./data.

    except Exception as e:
        print(f"RAG Initialization Failed: {e}")
        import traceback
        traceback.print_exc()

def _ingest_folder(folder_path):
    """Scans and ingests all documents inside a folder."""
    # Loops through files
    # Reads text from each file
    # Avoids re-indexing duplicates
    # Sends text for chunking + storage
    
    print(f"RAG: Scanning {folder_path}...")
    
    files_processed = 0
    
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        
        text = ""
        if filename.endswith(".pdf"):
            text = _read_pdf(file_path)
        elif filename.endswith(".txt") or filename.endswith(".md"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    text = f.read()
            except Exception:
                pass
        
        if text:
            # Check if this source exists
            existing = _collection.get(where={"source": filename})
            if existing and len(existing['ids']) > 0:
                continue

            # ❌ Re-embedding the same document again
            # ✔ Uses metadata (source) to detect duplicates

            _add_text_to_db(filename, text)
            files_processed += 1
            
    if files_processed > 0:
        print(f"RAG: Ingested {files_processed} new files.")

def _read_pdf(path):
    try:
        reader = PdfReader(path)
        text = ""
        for page in reader.pages:
            t = page.extract_text()
            if t: text += t + "\n"
        return text
    except Exception as e:
        print(f"RAG: Error reading PDF {path}: {e}")
        return ""

def _add_text_to_db(source_name, text):
    chunk_size = 1000
    overlap = 100

    # Large docs → bad embeddings
    # Chunking improves search accuracy
    # Overlap preserves context
    
    chunks = []
    metadatas = []
    ids = []
    
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        chunks.append(chunk)
        metadatas.append({"source": source_name})
        ids.append(f"{source_name}_{start}")
        
        start += (chunk_size - overlap)
        
    if chunks:
        _collection.add(documents=chunks, metadatas=metadatas, ids=ids)

        # Behind the scenes:
        # Text → embeddings
        # Embeddings → vector DB
        # Metadata stored alongside
        
        print(f"RAG: Indexed {source_name} ({len(chunks)} chunks)")

def search_knowledge_base(query: str, n_results: int = 3) -> str:    # This is what runs when a user asks a question.
    """Searches the vector DB for relevant context."""
    if _collection is None:
        initialize_rag()
    
    if _collection is None:
        return "Knowledge base unavailable."

    if _collection.count() == 0:
        return "No documents found in knowledge base."

    # Behind the scenes:
    # Query → embeddings
    # Cosine similarity search
    # Returns top N results/Top-K closest chunks returned
    
    results = _collection.query(
        query_texts=[query],
        n_results=n_results
    )
    
    if not results or not results['documents']:
         return "No relevant info found."

    docs = results['documents'][0]
    metas = results['metadatas'][0]
    
    context_data = []
    for i, doc in enumerate(docs):
        source = metas[i].get('source', 'unknown')
        context_data.append({"source": source, "content": doc})

    return json.dumps(context_data, indent=2)
